{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":218459,"sourceType":"datasetVersion","datasetId":93959}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd, numpy as np, os\ncsv_path = \"/kaggle/input/eeg-brainwave-dataset-feeling-emotions/emotions.csv\"\nassert os.path.exists(csv_path), f\"Not found: {csv_path}\"\ndf = pd.read_csv(csv_path)\nprint(df.shape); print(df.columns.tolist()[:40])\n\n# Pick target column robustly for this dataset\ncandidate_labels = ['label','emotion','liked','predefinedlabel','valence','arousal','dominance','target','class']\ny_name = next((c for c in candidate_labels if c in map(str.lower, df.columns)), None)\nif y_name is None:\n    # try exact-case fallback\n    y_name = next((c for c in candidate_labels if c in df.columns), None)\nassert y_name is not None, f\"No label column found in {candidate_labels}. Columns: {list(df.columns)[:30]}\"\n# normalize to actual case\ny_name = [c for c in df.columns if c.lower()==y_name][0]\n\n# drop obvious non-signal ID/meta columns if present\ndrop_meta = [c for c in ['id','user','gender','age','experiment_id','video'] if c in df.columns]\nX = df.drop(columns=[y_name] + drop_meta)\ny = df[y_name].astype(str)\nprint(\"Using label:\", y_name, \"| dropped meta:\", drop_meta, \"| X shape:\", X.shape)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-05T05:11:44.124248Z","iopub.execute_input":"2025-10-05T05:11:44.124565Z","iopub.status.idle":"2025-10-05T05:11:45.243936Z","shell.execute_reply.started":"2025-10-05T05:11:44.124541Z","shell.execute_reply":"2025-10-05T05:11:45.242947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns, matplotlib.pyplot as plt\ndf = df.dropna(axis=1, how='all')  # drop empty cols\n# Try to locate label column\nlabel_cols = [c for c in df.columns if c.lower() in ['label','emotion','target','class']]\nassert len(label_cols)>=1, \"Could not find a label column named label/emotion/target/class.\"\ny_name = label_cols[0]\ny = df[y_name].astype(str)\nX = df.drop(columns=[y_name])\n\nprint(\"Classes:\", y.value_counts(normalize=True).round(3).to_dict(), \"| n_features:\", X.shape[1])\n\n# Missing values\nmv = X.isna().mean()\nprint(\"Cols with >5% NaN:\", mv[mv>0.05].index.tolist()[:10])\nX = X.fillna(X.median(numeric_only=True))\n\n# Quick label bar\nsns.countplot(x=y); plt.title(\"Label distribution\"); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T05:11:45.245415Z","iopub.execute_input":"2025-10-05T05:11:45.245839Z","iopub.status.idle":"2025-10-05T05:11:46.660342Z","shell.execute_reply.started":"2025-10-05T05:11:45.245815Z","shell.execute_reply":"2025-10-05T05:11:46.659546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Heuristic: many columns with channel-like names or integer-step suffixes => time-series-like\ndef looks_like_timeseries(cols):\n    hits = 0\n    for c in cols[:2000]:\n        if any(ch in c.upper() for ch in ['AF','F','FC','T','P','O']) and any(ch.isdigit() for ch in c):\n            hits += 1\n        elif c.strip().split('_')[-1].isdigit():\n            hits += 1\n    return hits > 50 or X.shape[1] > 1000\n\nis_timeseries = looks_like_timeseries(list(X.columns))\nprint(\"Time-series-like:\", is_timeseries)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T05:11:46.661263Z","iopub.execute_input":"2025-10-05T05:11:46.661633Z","iopub.status.idle":"2025-10-05T05:11:46.672039Z","shell.execute_reply.started":"2025-10-05T05:11:46.661605Z","shell.execute_reply":"2025-10-05T05:11:46.671179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.signal import butter, filtfilt, iirnotch, welch\n\ndef butter_bandpass(low, high, fs, order=4):\n    b,a = butter(order, [low/(fs/2), high/(fs/2)], btype='band')\n    return b,a\n\ndef apply_notch(x, fs=128.0, f0=50.0, Q=30.0):\n    b,a = iirnotch(f0/(fs/2), Q)\n    return filtfilt(b,a, x)\n\ndef bandpowers(psd_f, psd_pxx, bands):\n    out={}\n    for name,(lo,hi) in bands.items():\n        idx = (psd_f>=lo) & (psd_f<=hi)\n        out[name] = np.trapz(psd_pxx[idx], psd_f[idx])\n    return out\n\ndef to_3d_timeseries(Xdf, fs_guess=128.0):\n    # Assume columns grouped per channel over time, try reshape by equal-length segments.\n    cols = Xdf.columns\n    # Infer per-row time length by greatest common divisor of column count across likely channel counts\n    guess_chs = [8,14,19,32,64]\n    nfeat = Xdf.shape[1]\n    for ch in guess_chs:\n        if nfeat % ch == 0:\n            T = nfeat//ch\n            return Xdf.values.reshape(len(Xdf), ch, T), ch, T, fs_guess\n    # fallback: treat as 1 channel series\n    return Xdf.values.reshape(len(Xdf), 1, nfeat), 1, nfeat, fs_guess\n\nif is_timeseries:\n    X3, n_ch, T, fs = to_3d_timeseries(X)\n    print(f\"Assumed shape: (n={X3.shape[0]}, ch={n_ch}, T={T}), fsâ‰ˆ{fs} Hz\")\n\n    # Filter + notch\n    bp_b, bp_a = butter_bandpass(1.0, 45.0, fs, order=4)\n    Xf = np.empty_like(X3, dtype=float)\n    for i in range(X3.shape[0]):\n        for c in range(n_ch):\n            sig = X3[i,c].astype(float)\n            try:\n                sig = filtfilt(bp_b, bp_a, sig)\n                sig = apply_notch(sig, fs=fs, f0=50.0, Q=30.0)\n            except Exception:\n                pass\n            Xf[i,c] = sig\n\n    # Bandpowers per channel\n    bands = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta\":(13,30),\"gamma\":(30,45)}\n    feats = []\n    for i in range(Xf.shape[0]):\n        row = {}\n        for c in range(n_ch):\n            f, pxx = welch(Xf[i,c], fs=fs, nperseg=min(256, T))\n            bp = bandpowers(f, pxx, bands)\n            for bname,val in bp.items():\n                row[f\"ch{c}_{bname}\"] = val\n        feats.append(row)\n    X_feat = pd.DataFrame(feats).replace([np.inf,-np.inf], np.nan).fillna(0.0)\nelse:\n    X_feat = X.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T05:11:46.673972Z","iopub.execute_input":"2025-10-05T05:11:46.674218Z","iopub.status.idle":"2025-10-05T05:12:04.787245Z","shell.execute_reply.started":"2025-10-05T05:11:46.674200Z","shell.execute_reply":"2025-10-05T05:12:04.786230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt, numpy as np, seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nXz = StandardScaler().fit_transform(X_feat)\n\n# PCA scatter\npca = PCA(n_components=2, random_state=0).fit_transform(Xz)\nplt.figure(); sns.scatterplot(x=pca[:,0], y=pca[:,1], hue=y, s=12)\nplt.title(\"PCA(2) by label\"); plt.legend(bbox_to_anchor=(1.05,1)); plt.show()\n\n# UMAP scatter\nimport umap\num = umap.UMAP(n_neighbors=30, min_dist=0.1, random_state=0).fit_transform(Xz)\nplt.figure(); sns.scatterplot(x=um[:,0], y=um[:,1], hue=y, s=12)\nplt.title(\"UMAP(2) by label\"); plt.legend(bbox_to_anchor=(1.05,1)); plt.show()\n\n# If time-series: plot an example trace and PSD\nif is_timeseries:\n    i0 = 0\n    fig,ax=plt.subplots(); \n    ax.plot(Xf[i0,0]); ax.set_title(\"Example channel 0 (filtered)\"); plt.show()\n    f,pxx = welch(Xf[i0,0], fs=fs, nperseg=min(256,T))\n    plt.figure(); plt.semilogy(f,pxx); plt.title(\"Example PSD\"); plt.xlabel(\"Hz\"); plt.ylabel(\"PSD\"); plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T05:12:04.788265Z","iopub.execute_input":"2025-10-05T05:12:04.788607Z","iopub.status.idle":"2025-10-05T05:12:12.827567Z","shell.execute_reply.started":"2025-10-05T05:12:04.788576Z","shell.execute_reply":"2025-10-05T05:12:12.826662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_enc = le.fit_transform(y)\nclasses = le.classes_\nprint(\"Classes:\", list(classes))\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T05:12:12.828608Z","iopub.execute_input":"2025-10-05T05:12:12.828887Z","iopub.status.idle":"2025-10-05T05:12:12.834920Z","shell.execute_reply.started":"2025-10-05T05:12:12.828868Z","shell.execute_reply":"2025-10-05T05:12:12.834071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nimport numpy as np\n\ndef cv_eval_model(model, X, y, skf): #cross validation function\n    accs, f1s = [], []\n    cms = np.zeros((len(np.unique(y)), len(np.unique(y))), dtype=int)\n    for tr,te in skf.split(X,y):\n        model.fit(X[tr], y[tr])\n        yp = model.predict(X[te])\n        accs.append(accuracy_score(y[te], yp))\n        f1s.append(f1_score(y[te], yp, average='weighted'))\n        cms += confusion_matrix(y[te], yp, labels=np.unique(y))\n    return np.mean(accs), np.std(accs), np.mean(f1s), cms\n#Performs 5-fold stratified cross-validation:\n#Splits data into 5 parts maintaining class balance.\n#Trains on 4 parts, tests on 1.\n#Records accuracy, weighted F1, and confusion matrix per fold.\n#Returns their averages\n\nXn = X_feat.values.astype(np.float32) #data preparation\n#Converts the features (X_feat) into a NumPy array of type float32, which most ML libraries use.\n\nxgb_clf = xgb.XGBClassifier(\n    n_estimators=400, max_depth=6, learning_rate=0.05,\n    subsample=0.9, colsample_bytree=0.8, reg_lambda=1.0,\n    tree_method=\"hist\", random_state=42\n)\nacc_mu, acc_sd, f1_mu, cm = cv_eval_model(xgb_clf, Xn, y_enc, skf) #Trains and evaluates the model.\n#Outputs mean accuracy Â± std and average F1 score.\nprint(f\"[XGBoost] 5-fold Acc: {acc_mu:.3f} Â± {acc_sd:.3f} | F1w: {f1_mu:.3f}\")\nprint(\"Confusion matrix:\\n\", cm)\n\n#This builds an XGBoost model â€” an ensemble of decision trees trained via gradient boosting.\n#Each tree corrects the mistakes of the previous ones.\n\n# Feature importance plot (top 20)\nimportances = xgb_clf.fit(Xn, y_enc).feature_importances_\nidx = np.argsort(importances)[::-1][:20]\nplt.figure(figsize=(6,5))\nsns.barplot(x=importances[idx], y=np.array(X_feat.columns)[idx])\nplt.title(\"Top features (XGB)\"); plt.tight_layout(); plt.show()\n#Retrains XGBoost once on all data to extract feature importances and plots the top 20.\n#Helps identify which EEG features most strongly drive the emotion prediction.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T05:12:12.835889Z","iopub.execute_input":"2025-10-05T05:12:12.836140Z","iopub.status.idle":"2025-10-05T05:12:24.867454Z","shell.execute_reply.started":"2025-10-05T05:12:12.836115Z","shell.execute_reply":"2025-10-05T05:12:24.866557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\n\nmodels = {\n    \"XGBoost\": xgb.XGBClassifier(\n        n_estimators=400, max_depth=6, learning_rate=0.05,\n        subsample=0.9, colsample_bytree=0.8, random_state=42, tree_method=\"hist\"\n    ),\n    \"RandomForest\": RandomForestClassifier(\n        n_estimators=300, max_depth=None, random_state=42, n_jobs=-1\n    ),\n    \"SVM (RBF)\": SVC(kernel='rbf', C=1.0, gamma='scale'),\n    \"LogisticRegression\": LogisticRegression(max_iter=1000, n_jobs=-1),\n    \"LightGBM\": lgb.LGBMClassifier(\n        n_estimators=400, learning_rate=0.05, num_leaves=31, random_state=42\n    )\n}\n\nresults = []\nfor name, model in models.items():\n    acc_mu, acc_sd, f1_mu, cm = cv_eval_model(model, Xn, y_enc, skf)\n    results.append((name, acc_mu, acc_sd, f1_mu))\n    print(f\"[{name}] 5-fold Acc: {acc_mu:.3f} Â± {acc_sd:.3f} | F1w: {f1_mu:.3f}\")\n\n# Summarize\nresults_df = pd.DataFrame(results, columns=[\"Model\",\"Acc_mean\",\"Acc_std\",\"F1_weighted\"])\nprint(\"\\nModel comparison:\\n\", results_df.sort_values(\"Acc_mean\", ascending=False))\nsns.barplot(x=\"Acc_mean\", y=\"Model\", data=results_df)\nplt.title(\"Model accuracy comparison (5-fold CV)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T05:12:24.868560Z","iopub.execute_input":"2025-10-05T05:12:24.868838Z","iopub.status.idle":"2025-10-05T05:12:59.830954Z","shell.execute_reply.started":"2025-10-05T05:12:24.868817Z","shell.execute_reply":"2025-10-05T05:12:59.830125Z"}},"outputs":[],"execution_count":null}]}